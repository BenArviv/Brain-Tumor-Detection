{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b670ad-e589-47a5-b29b-6d64f0bbf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bf9df1-8d38-40cd-9c6e-78cd342fe189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo2bbox(bboxes):\n",
    "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
    "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
    "    return xmin, ymin, xmax, xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42052d0f-e2b9-463e-9c39-8494071e65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image_paths, label_paths, num_samples=4):\n",
    "    all_images = []\n",
    "    all_images.extend(glob.glob(image_paths+'/*.jpg'))\n",
    "    all_images.extend(glob.glob(image_paths+'/*.JPG'))\n",
    "    all_images.sort()\n",
    "    num_images = len(all_images)\n",
    "    plt.figure(figsize=(15,12))\n",
    "    for i in range(num_samples):\n",
    "        j = random.randint(0,num_images-1)\n",
    "        image_name = all_images[j]\n",
    "        image_name = '.'.join(image_name.split(os.path.sep)[-1].split('.')[:-1])\n",
    "        image = cv2.imread(all_images[j])\n",
    "        with open(os.path.join(label_paths, image_name+'.txt'), 'r') as f:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            label_lines = f.readlines()\n",
    "            for label_line in label_lines:\n",
    "                label = label_line[0]\n",
    "                bbox_string = label_line [2:]\n",
    "                x_c, y_c, w, h = bbox_string.split(' ')\n",
    "                x_c = float(x_c)\n",
    "                y_c = float(y_c)\n",
    "                w = float(w)\n",
    "                h = float(h)\n",
    "                bboxes.append([x_c, y_c, w ,h])\n",
    "                labels.append(label)\n",
    "            result_image = plot_box(image, bboxes, labels)\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            plt.imshow(result_image[:,:,::-1])\n",
    "            plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=1)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60420-2e00-46d9-90b8-831272912a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tumor_v8.yaml\n",
    "path: '/Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection/new'\n",
    "train: 'train/images'\n",
    "val: 'valid/images'\n",
    "test: 'test/images'\n",
    "\n",
    "#class names\n",
    "names:\n",
    "    0: 'type1'\n",
    "    1: 'type2'\n",
    "    2: 'type3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e5c7d-9629-4d2d-8658-bd47aee410f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab806aef-1d00-4855-8eba-b5ece43e3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71a3c1-86f6-4637-bcf4-01808d064c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your trained YOLO model and `test_images` is a list of your test images\n",
    "\n",
    "for image in test_images:\n",
    "    # Run the model on the test image\n",
    "    boxes, confidences, class_ids = model.predict(image)\n",
    "\n",
    "    # Now `boxes` contains the bounding boxes predicted by the model,\n",
    "    # `confidences` contains the confidence scores for each box, and\n",
    "    # `class_ids` contains the class IDs for each box.\n",
    "\n",
    "    # You can compare these predictions to the ground truth to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a84f2-5598-4bca-b330-b4b12e49712c",
   "metadata": {},
   "source": [
    "# new code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8bef808-c89b-4218-8fa9-824bc8f85d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since using only label 1 we need to rewrite the label files\n",
    "#by removing label 0 and 2 and the scans without any tumor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2cd0a5d2-edf6-46f7-a1c0-88a4ebc1a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data_path):\n",
    "    os.makedirs('newTrain',exist_ok=True)\n",
    "    os.makedirs(os.path.join(os.getcwd(),'newTrain', 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'newTrain', 'labels'), exist_ok=True)\n",
    "\n",
    "    labels_path = os.path.join(os.getcwd(), 'train', 'labels')\n",
    "    img_path = os.path.join(os.getcwd(), 'train', 'images')\n",
    "\n",
    "    new_labels_path = os.path.join(os.getcwd(), 'newTrain', 'labels')\n",
    "    new_images_path = os.path.join(os.getcwd(), 'newTrain', 'images')\n",
    "    \n",
    "    for file_name in os.listdir(labels_path):\n",
    "        file_path  = os.path.join(labels_path, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            image_file_name = file_name.replace('.txt', '.jpg')\n",
    "            image_file_path = os.path.join(img_path, image_file_name)\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                shutil.copy(file_path, new_labels_path)\n",
    "                shutil.copy(image_file_path, new_images_path)\n",
    "            else: \n",
    "                lines_id = [line for line in lines if line.startswith('1')]\n",
    "                if lines_id:\n",
    "                    with open(os.path.join(new_labels_path, file_name), 'w') as f: \n",
    "                        lines_id = lines_id[0].split(' ')\n",
    "                        lines_id[0] = '0 '\n",
    "                        ' '.join(lines_id)\n",
    "                        f.writelines(lines_id)\n",
    "                        shutil.copy(image_file_path, new_images_path)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb7cc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data2(data_path, dir1, dir2):\n",
    "    os.makedirs('newTrain',exist_ok=True)\n",
    "    os.makedirs(os.path.join(os.getcwd(),dir2, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(os.getcwd(), dir2, 'labels'), exist_ok=True)\n",
    "\n",
    "    labels_path = os.path.join(os.getcwd(), dir1, 'labels')\n",
    "    img_path = os.path.join(os.getcwd(), dir1, 'images')\n",
    "\n",
    "    new_labels_path = os.path.join(os.getcwd(), dir2, 'labels')\n",
    "    new_images_path = os.path.join(os.getcwd(), dir2, 'images')\n",
    "    \n",
    "    for file_name in os.listdir(labels_path):\n",
    "        file_path  = os.path.join(labels_path, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            image_file_name = file_name.replace('.txt', '.jpg')\n",
    "            image_file_path = os.path.join(img_path, image_file_name)\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                shutil.copy(file_path, new_labels_path)\n",
    "                shutil.copy(image_file_path, new_images_path)\n",
    "            else: \n",
    "                lines_id = [line for line in lines if line.startswith('1')]\n",
    "                if lines_id:\n",
    "                    with open(os.path.join(new_labels_path, file_name), 'w') as f: \n",
    "                        lines_id = lines_id[0].split(' ')\n",
    "                        lines_id[0] = '0 '\n",
    "                        lines_id = ' '.join(lines_id)\n",
    "                        lines_id = lines_id.strip()\n",
    "                        f.writelines(lines_id)\n",
    "                        shutil.copy(image_file_path, new_images_path)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b80f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path ='/Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection/new'\n",
    "clean_data2(my_path, 'train', 'newTrain')\n",
    "clean_data2(my_path, 'valid', 'newValid')\n",
    "clean_data2(my_path, 'test', 'newTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d92189f0-805b-4641-a07e-d26dbb6fe2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data('/Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "979118f9-80f8-477f-bb49-db46ed07d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now after the data is cleaned we can start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9bb448ae-ec5d-428b-8314-50638c39786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tumor_v8.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile tumor_v8.yaml\n",
    "path: '/Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection'\n",
    "train: 'newTrain/images'\n",
    "val: 'newValid/images'\n",
    "test: 'newTest/images'\n",
    "nc: 1\n",
    "\n",
    "#class names\n",
    "names:\n",
    "    0: '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "12f31338-c2ad-478a-8399-3a8db8174c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "059dc018-40bf-4813-8608-14f578d783e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 8\n",
    "BATCH = 256\n",
    "IMG_SIZE = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad4068a4-6353-4883-8ef9-65a9769b0266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.3 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.2 🚀 Python-3.11.0 torch-2.2.0 CPU (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=tumor_v8.yaml, epochs=8, time=None, patience=100, batch=256, imgsz=160, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=output17, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/output17\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/output17', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection/newTrain/labels... 6824 images, 79 backgrounds, 0 corrupt: 100%|██████████| 6824/6824 [00:01<00:00, 3634.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection/newTrain/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection/newValid/labels... 1961 images, 17 backgrounds, 0 corrupt: 100%|██████████| 1961/1961 [00:00<00:00, 3246.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/benarviv/Documents/OpenU/2024א/Data Science Workshop/Data Science Project/Brain-Tumor-Detection/newValid/labels.cache\n",
      "Plotting labels to runs/detect/output17/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.002), 83 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 160 train, 160 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/output17\u001b[0m\n",
      "Starting training for 8 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/8         0G      1.857      1.901      1.574        329        160: 100%|██████████| 27/27 [12:42<00:00, 28.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:40<00:00, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.566      0.596      0.579      0.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/8         0G      1.578      1.148      1.423        333        160: 100%|██████████| 27/27 [12:38<00:00, 28.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:35<00:00, 23.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944       0.31      0.143      0.125     0.0488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/8         0G      1.559      1.112      1.416        315        160: 100%|██████████| 27/27 [12:32<00:00, 27.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:29<00:00, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.554      0.425      0.436      0.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/8         0G      1.555      1.094      1.415        314        160: 100%|██████████| 27/27 [12:39<00:00, 28.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:33<00:00, 23.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.713      0.567      0.606      0.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/8         0G      1.507      1.049      1.387        306        160: 100%|██████████| 27/27 [12:58<00:00, 28.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:33<00:00, 23.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.771      0.586      0.634      0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        6/8         0G      1.446     0.9612      1.352        285        160: 100%|██████████| 27/27 [12:41<00:00, 28.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:34<00:00, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.829      0.637      0.716      0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        7/8         0G      1.376     0.9171      1.315        324        160: 100%|██████████| 27/27 [12:35<00:00, 27.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:29<00:00, 22.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.841       0.65      0.736      0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        8/8         0G      1.322     0.8609      1.288        313        160: 100%|██████████| 27/27 [12:36<00:00, 28.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:33<00:00, 23.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.883      0.692      0.771      0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 epochs completed in 1.904 hours.\n",
      "Optimizer stripped from runs/detect/output17/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/output17/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/detect/output17/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.2 🚀 Python-3.11.0 torch-2.2.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:35<00:00, 23.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1961       1944      0.883      0.692      0.771      0.467\n",
      "Speed: 0.1ms preprocess, 47.9ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/output17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(\n",
    "    data = 'tumor_v8.yaml',\n",
    "    imgsz = IMG_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    batch = BATCH,\n",
    "    name = 'output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1939d-816e-494e-bcbc-972ce8707759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
